import cv2
import numpy as np
from mtcnn import MTCNN
from torchvision import transforms
from PIL import Image
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Lazy init for MTCNN to reduce import-time overhead
mtcnn_detector = None

def _get_mtcnn():
    global mtcnn_detector
    if mtcnn_detector is None:
        mtcnn_detector = MTCNN()
    return mtcnn_detector

def detect_with_haar_cascade(img, target_size=(224, 224)):
    """
    Fallback: ‡πÉ‡∏ä‡πâ Haar Cascade ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö detect ‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏ô‡∏à‡∏£‡∏¥‡∏á (‡∏£‡∏±‡∏ö numpy image)
    """
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(
        gray,
        scaleFactor=1.1,
        minNeighbors=5,
        minSize=(40, 40)
    )
    if len(faces) == 0:
        print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û (Haar Cascade)")
        return None, None

    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏£‡∏≠‡∏õ
    x, y, w, h = max(faces, key=lambda b: b[2] * b[3])
    print("üë§ ‡∏û‡∏ö‡∏´‡∏ô‡πâ‡∏≤ (Haar Cascade) ‡∏Ç‡∏ô‡∏≤‡∏î:", w, h)

    H, W = img.shape[0], img.shape[1]
    # ‡πÉ‡∏ä‡πâ padding ‡∏ï‡∏≤‡∏°‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤ (‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏û‡∏≠‡∏î‡∏µ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏á‡∏ó‡∏µ‡πà)
    pad = int(round(0.2 * max(w, h)))
    x1 = max(0, int(x - pad))
    y1 = max(0, int(y - pad))
    x2 = min(W, int(x + w + pad))
    y2 = min(H, int(y + h + pad))

    if x2 <= x1 or y2 <= y1:
        return None, None

    face_crop = img[y1:y2, x1:x2]
    if face_crop.size == 0:
        return None, None
    face_resized = cv2.resize(face_crop, target_size)
    face_rgb = cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB)

    return face_rgb, (int(x), int(y), int(w), int(h))

def detect_and_crop_face(img, target_size=(224, 224), use_anime_detection=False):

    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    if use_anime_detection:
        try:
            detector = _get_mtcnn()
            faces = detector.detect_faces(img_rgb)
            if len(faces) == 0:
                print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û (MTCNN)")
                return detect_with_haar_cascade(img, target_size)
            # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î/‡πÉ‡∏´‡∏ç‡πà‡∏™‡∏∏‡∏î
            face_data = max(
                faces,
                key=lambda f: (f.get('confidence', 0), (f['box'][2] * f['box'][3]))
            )
            x, y, w, h = face_data['box']
            # clamp ‡πÅ‡∏•‡∏∞‡∏õ‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô int
            H, W = img_rgb.shape[0], img_rgb.shape[1]
            x = max(0, int(round(x)))
            y = max(0, int(round(y)))
            w = max(1, int(round(w)))
            h = max(1, int(round(h)))
            confidence = face_data.get('confidence', None)
            if confidence is not None:
                try:
                    print(f"üé≠ ‡∏û‡∏ö‡∏´‡∏ô‡πâ‡∏≤ (MTCNN) - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à: {confidence:.2f}")
                except Exception:
                    print("üé≠ ‡∏û‡∏ö‡∏´‡∏ô‡πâ‡∏≤ (MTCNN)")

        except Exception as e:
            print(f"‚ö†Ô∏è MTCNN error: {e}")
            print("üîÑ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÉ‡∏ä‡πâ Haar Cascade")
            return detect_with_haar_cascade(img, target_size)
    else:
        return detect_with_haar_cascade(img, target_size)

    # Crop ‡∏´‡∏ô‡πâ‡∏≤ (padding ‡∏ï‡∏≤‡∏°‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤)
    H, W = img_rgb.shape[0], img_rgb.shape[1]
    pad = int(round(0.2 * max(w, h)))
    x1 = max(0, x - pad)
    y1 = max(0, y - pad)
    x2 = min(W, x + w + pad)
    y2 = min(H, y + h + pad)

    if x2 <= x1 or y2 <= y1:
        return None, None

    face_crop = img_rgb[y1:y2, x1:x2]
    if face_crop.size == 0:
        return None, None

    face_resized = cv2.resize(face_crop, target_size)
    return face_resized, (int(x), int(y), int(w), int(h))

# Reuse a single transform definition from model_image to avoid divergence
try:
    from .model_image import transform as image_transform
except Exception:
    try:
        from Backend.model_image import transform as image_transform
    except Exception:
        from model_image import transform as image_transform

# Convenience wrappers for reuse elsewhere
def crop_face(img, target_size=(224, 224), use_anime_detection=True):
    """
    ‡∏Ñ‡∏£‡∏≠‡∏õ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û numpy (BGR) ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏õ‡πá‡∏ô target_size, ‡∏Ñ‡∏∑‡∏ô (face_rgb, coords)
    """
    return detect_and_crop_face(img, target_size=target_size, use_anime_detection=use_anime_detection)

def to_grayscale_224(face_image):
    """
    ‡πÅ‡∏õ‡∏•‡∏á‡∏†‡∏≤‡∏û‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤ (RGB numpy ‡∏´‡∏£‡∏∑‡∏≠ BGR ‡∏Å‡πá‡πÑ‡∏î‡πâ) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≤‡∏ß‡∏î‡∏≥ 3 ‡πÅ‡∏ä‡∏ô‡πÄ‡∏ô‡∏• ‡∏Ç‡∏ô‡∏≤‡∏î 224x224 (numpy RGB)
    ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏´‡∏•‡∏±‡∏á preprocess
    """
    # Ensure RGB numpy
    if isinstance(face_image, np.ndarray):
        if len(face_image.shape) == 3 and face_image.shape[2] == 3:
            # assume RGB already; if comes as BGR from OpenCV, convert before calling this helper
            rgb = cv2.resize(face_image, (224, 224))
            gray = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)
            gray3 = np.stack([gray, gray, gray], axis=2)
            return gray3
    # Fallback via PIL path
    pil_img = Image.fromarray(face_image)
    pil_gray = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.Grayscale(num_output_channels=3)
    ])(pil_img)
    return np.array(pil_gray)

def preprocess_face_for_model(face_image, model):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ó‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤ (RGB numpy)
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ torch.Tensor ‡∏ö‡∏ô‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
    """
    face_pil = Image.fromarray(face_image)
    img_t = image_transform(face_pil).unsqueeze(0)
    model_device = next(model.parameters()).device
    return img_t.to(model_device)

def predict_face_image(face_image, model, class_names):
    face_pil = Image.fromarray(face_image)
    img_t = image_transform(face_pil).unsqueeze(0)
    # Send to the same device as the model to avoid CPU/CUDA mismatch
    model_device = next(model.parameters()).device
    img_t = img_t.to(model_device)
    with torch.no_grad():
        outputs = model(img_t)
        probabilities = torch.softmax(outputs, dim=1)
        confidence, pred = torch.max(probabilities, 1)
    
    return {
        "emotion": class_names[pred.item()],
        "confidence": confidence.item(),
        "probabilities": {class_names[i]: probabilities[0][i].item() for i in range(len(class_names))}
    }

# === Path-based helpers (for quick local testing/CLI) ===
def detect_and_crop_face_from_path(image_path, target_size=(224, 224)):
    """
    Detect ‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏•‡∏∞ crop ‡∏à‡∏≤‡∏Å‡∏û‡∏≤‡∏ò‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏û ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Haar Cascade (‡πÑ‡∏°‡πà‡∏û‡∏∂‡πà‡∏á Colab/files.upload)
    ‡∏Ñ‡∏∑‡∏ô (face_rgb_224, (x, y, w, h)) ‡∏´‡∏£‡∏∑‡∏≠ (None, None) ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏´‡∏ô‡πâ‡∏≤
    """
    img = cv2.imread(image_path)
    if img is None:
        return None, None
    return detect_with_haar_cascade(img, target_size=target_size)

def predict_image_from_path(image_path, model, class_names):
    """
    ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏à‡∏≤‡∏Å‡∏û‡∏≤‡∏ò‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏û‡πÄ‡∏ï‡πá‡∏° (‡∏à‡∏∞‡∏ñ‡∏π‡∏Å resize+grayscale 224x224 ‡∏ï‡∏≤‡∏° transform)
    ‡πÉ‡∏ä‡πâ device ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
    """
    image = Image.open(image_path).convert('RGB')
    img_t = image_transform(image).unsqueeze(0)
    model_device = next(model.parameters()).device
    img_t = img_t.to(model_device)
    with torch.no_grad():
        outputs = model(img_t)
        _, pred = torch.max(outputs, 1)
    return class_names[pred.item()]
